==========================================
HW3 Multi-Node Benchmark Run (2 nodes)
Nodes allocated: 2
Tasks per node: 8
Total tasks: 16
Date: Tue Nov 18 21:09:41 EST 2025
==========================================

Loading MPI module...

Building MPI executable...
rm -f hw3 hw3_openmp *.o
mpic++ -O3 -march=native -mavx2 -mfma -std=c++11 -ffast-math -funroll-loops -ftree-vectorize -o hw3 hw3.cpp 

Running multi-node benchmarks (16 processes across 2 nodes)...
Multi-Node MPI Benchmark Runner
==================================================

This script extends existing single-node results with multi-node data.
Requires: MPI job with multiple nodes allocated

=== Running Multi-Node Strong Scaling ===
Matrix sizes: [4000, 8000, 16000]
Process counts: [16]
Testing N=4000, procs=16... Time=4612.00us, Perf=6.94 Gflop/s, Speedup=0.71x, Eff=4.4%
Testing N=8000, procs=16... Time=5540.00us, Perf=23.10 Gflop/s, Speedup=2.18x, Eff=13.6%
Testing N=16000, procs=16... Time=10229.00us, Perf=50.05 Gflop/s, Speedup=1.00x, Eff=100.0%

Updated strong_scaling_results.csv with 25 total entries

=== Running Multi-Node Weak Scaling ===
Base work per process: ~1M elements
Process counts: [16]
Testing procs=16, N=4000... Time=4190.00us, Perf=7.64 Gflop/s, Eff=100.0%

Regenerating PDF report with multi-node data...
Loading performance data...
Loaded OpenMP comparison data: 16 entries
Generating charts...
Generated strong_scaling_single_node.png
Generated strong_scaling_multi_node.png
Creating PDF report...
Report successfully generated: hw3.pdf

=== Report generation complete ===
Output: hw3.pdf

==========================================
Multi-node job complete!
Check hw3.pdf for complete results
==========================================
